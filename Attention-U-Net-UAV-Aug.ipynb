{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76323663-c0d3-472d-8c35-98c22f244b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Reshape, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define image dimensions and channels\n",
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 512\n",
    "IMG_CHANNELS = 3\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "num_classes = 1  # Single class segmentation\n",
    "SEED = 40\n",
    "\n",
    "train_image_dir = '/home/syam.varnatt/THESIS/images/train/images'\n",
    "train_mask_dir = '/home/syam.varnatt/THESIS/images/train/masks'\n",
    "val_image_dir = '/home/syam.varnatt/THESIS/images/val/images'\n",
    "val_mask_dir = '/home/syam.varnatt/THESIS/images/val/masks'\n",
    "SAVE_DIRECTORY = '/home/syam.varnatt/THESIS/attention-unet/aug/results'\n",
    "\n",
    "def load_dataset(image_dir, mask_dir, img_size):\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    for image_name in tqdm(os.listdir(image_dir)):\n",
    "        if image_name.lower().endswith(('.jpeg', '.jpg')):\n",
    "            base_name = os.path.splitext(image_name)[0]\n",
    "            mask_name = f\"{base_name}_mask.jpg\"\n",
    "            \n",
    "            img_path = os.path.join(image_dir, image_name)\n",
    "            mask_path = os.path.join(mask_dir, mask_name)\n",
    "            \n",
    "            if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "                try:\n",
    "                    image = load_img(img_path, target_size=img_size)\n",
    "                    mask = load_img(mask_path, target_size=img_size, color_mode=\"grayscale\")\n",
    "                    \n",
    "                    image = img_to_array(image)\n",
    "                    mask = img_to_array(mask)\n",
    "                    \n",
    "                    images.append(image)\n",
    "                    masks.append(mask)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_name}: {e}\")\n",
    "            else:\n",
    "                print(f\"Mask not found for image: {image_name}\")\n",
    "    \n",
    "    images = np.array(images) / 255.0\n",
    "    masks = np.array(masks) / 255.0\n",
    "    masks = (masks > 0.5).astype(np.uint8)\n",
    "    \n",
    "    return images, masks\n",
    "\n",
    "def display_images_and_masks(original_images, original_masks, augmented_images, augmented_masks, num=3):\n",
    "    \"\"\"Display original and augmented images and masks in a grid layout.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num):\n",
    "        # Display original images\n",
    "        plt.subplot(4, num, i + 1)\n",
    "        plt.imshow(original_images[i])\n",
    "        plt.title(f'Original Image {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(4, num, i + 1 + num)\n",
    "        plt.imshow(original_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Original Mask {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display augmented images\n",
    "        plt.subplot(4, num, i + 1 + num*2)\n",
    "        plt.imshow(augmented_images[i])\n",
    "        plt.title(f'Augmented Image {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(4, num, i + 1 + num*3)\n",
    "        plt.imshow(augmented_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Augmented Mask {i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Data augmentation setup\n",
    "data_gen_args = dict(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0,\n",
    "    rescale=1.0\n",
    ")\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Load datasets\n",
    "train_images, train_masks = load_dataset(train_image_dir, train_mask_dir, (IMG_HEIGHT, IMG_WIDTH))\n",
    "print(\"Training dataset loaded completely\")\n",
    "val_images, val_masks = load_dataset(val_image_dir, val_mask_dir, (IMG_HEIGHT, IMG_WIDTH))\n",
    "print(\"Validation dataset loaded completely\")\n",
    "\n",
    "# Ensure the same seed is used for images and masks\n",
    "image_datagen.fit(train_images, augment=True, seed=SEED)\n",
    "mask_datagen.fit(train_masks, augment=True, seed=SEED)\n",
    "\n",
    "# Generate augmented images and masks\n",
    "augmented_images, augmented_masks = [], []\n",
    "for i in range(len(train_images)):\n",
    "    img = train_images[i].reshape((1,) + train_images[i].shape)\n",
    "    mask = train_masks[i].reshape((1,) + train_masks[i].shape)\n",
    "    \n",
    "    img_gen = image_datagen.flow(img, batch_size=1, seed=SEED)\n",
    "    mask_gen = mask_datagen.flow(mask, batch_size=1, seed=SEED)\n",
    "    \n",
    "    augmented_images.append(img_gen[0][0])\n",
    "    augmented_masks.append(mask_gen[0][0])\n",
    "\n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_masks = np.array(augmented_masks)\n",
    "\n",
    "# Concatenate the original and augmented datasets\n",
    "all_images = np.concatenate((train_images, augmented_images), axis=0)\n",
    "all_masks = np.concatenate((train_masks, augmented_masks), axis=0)\n",
    "\n",
    "total_images = len(all_images)\n",
    "total_masks = len(all_masks)\n",
    "\n",
    "print(\"Total images : \", total_images)\n",
    "print(\"Total masks : \", total_masks)\n",
    "# Display images and masks before and after augmentation\n",
    "display_images_and_masks(train_images, train_masks, augmented_images, augmented_masks)\n",
    "\n",
    "# Load datasets\n",
    "train_images, train_masks = load_dataset(train_image_dir, train_mask_dir, (IMG_HEIGHT, IMG_WIDTH))\n",
    "print(\"Training dataset loaded completely\")\n",
    "val_images, val_masks = load_dataset(val_image_dir, val_mask_dir, (IMG_HEIGHT, IMG_WIDTH))\n",
    "print(\"Validation dataset loaded completely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927f206-e017-4011-b016-530294c99498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(x, g, inter_channel):\n",
    "    theta_x = layers.Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    phi_g = layers.Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(g)\n",
    "    f = layers.Activation('relu')(layers.add([theta_x, phi_g]))\n",
    "    psi_f = layers.Conv2D(1, (1, 1), strides=(1, 1), padding='same')(f)\n",
    "    rate = layers.Activation('sigmoid')(psi_f)\n",
    "    att_x = layers.multiply([x, rate])\n",
    "    return att_x\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = layers.Conv2D(filters, kernel_size=kernel_size, padding=padding, strides=strides)(x)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.Activation(\"relu\")(conv)\n",
    "    return conv\n",
    "\n",
    "def attention_unet(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = conv_block(inputs, 64)\n",
    "    conv1 = conv_block(conv1, 64)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = conv_block(pool1, 128)\n",
    "    conv2 = conv_block(conv2, 128)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = conv_block(pool2, 256)\n",
    "    conv3 = conv_block(conv3, 256)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = conv_block(pool3, 512)\n",
    "    conv4 = conv_block(conv4, 512)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv5 = conv_block(pool4, 1024)\n",
    "    conv5 = conv_block(conv5, 1024)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "    att6 = attention_block(conv4, up6, 512)\n",
    "    merge6 = layers.concatenate([up6, att6], axis=3)\n",
    "    conv6 = conv_block(merge6, 512)\n",
    "    conv6 = conv_block(conv6, 512)\n",
    "\n",
    "    up7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "    att7 = attention_block(conv3, up7, 256)\n",
    "    merge7 = layers.concatenate([up7, att7], axis=3)\n",
    "    conv7 = conv_block(merge7, 256)\n",
    "    conv7 = conv_block(conv7, 256)\n",
    "\n",
    "    up8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "    att8 = attention_block(conv2, up8, 128)\n",
    "    merge8 = layers.concatenate([up8, att8], axis=3)\n",
    "    conv8 = conv_block(merge8, 128)\n",
    "    conv8 = conv_block(conv8, 128)\n",
    "\n",
    "    up9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
    "    att9 = attention_block(conv1, up9, 64)\n",
    "    \n",
    "    merge9 = layers.concatenate([up9, att9], axis=3)\n",
    "    conv9 = conv_block(merge9, 64)\n",
    "    conv9 = conv_block(conv9, 64)\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    return model\n",
    "    \n",
    "model = attention_unet(input_shape, num_classes)\n",
    "# Compile the model with appropriate loss and metrics\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d2cc2-2103-48ad-b584-13015bccca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_images, val_masks, save_dir, num_samples=3):\n",
    "        super(VisualizationCallback, self).__init__()\n",
    "        self.val_images = val_images\n",
    "        self.val_masks = val_masks\n",
    "        self.save_dir = save_dir\n",
    "        self.num_samples = num_samples  # Number of samples to visualize per epoch\n",
    "        os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Randomly select a few samples to visualize predictions\n",
    "        idxs = np.random.choice(len(self.val_images), self.num_samples, replace=False)\n",
    "        images_to_show = self.val_images[idxs]\n",
    "        masks_to_show = self.val_masks[idxs]\n",
    "        \n",
    "        # Use the model attribute directly, which is set automatically by Keras\n",
    "        predictions = self.model.predict(images_to_show)\n",
    "\n",
    "        # Plot and save the images, masks, and predictions\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i in range(self.num_samples):\n",
    "            plt.subplot(self.num_samples, 3, i * 3 + 1)\n",
    "            plt.imshow(images_to_show[i])\n",
    "            plt.title(f'Input Image {i+1}')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(self.num_samples, 3, i * 3 + 2)\n",
    "            plt.imshow(masks_to_show[i].squeeze(), cmap='gray')\n",
    "            plt.title(f'True Mask {i+1}')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(self.num_samples, 3, i * 3 + 3)\n",
    "            plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
    "            plt.title(f'Predicted Mask {i+1}')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Save the plot to file\n",
    "        plt.savefig(os.path.join(self.save_dir, f'epoch_{epoch+1:02d}_predictions.png'))\n",
    "        plt.close()\n",
    "        print(f\"Saved visualization for epoch {epoch+1}.\")\n",
    "\n",
    "# Instantiate the visualization callback\n",
    "visualization_callback = VisualizationCallback(val_images, val_masks, SAVE_DIRECTORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f1764-5a39-439a-88ad-c17a116c6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingTimeCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - self.start_time\n",
    "        print(f\"Training Time: {training_time / 60:.2f} minutes\")\n",
    "\n",
    "# Create an instance of the callback\n",
    "training_time_callback = TrainingTimeCallback()\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('attention-unet.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    all_images, all_masks,\n",
    "    validation_data=(val_images, val_masks),\n",
    "    batch_size=8,\n",
    "    epochs=150,\n",
    "    callbacks=[early_stopping, checkpoint, training_time_callback, visualization_callback],\n",
    "    verbose=1 \n",
    ")\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Save the training history to an Excel file\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_excel(os.path.join('training_history.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a72202-ed2c-41b7-b34a-fa510b499c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on validation set\n",
    "val_preds = model.predict(val_images)\n",
    "\n",
    "# Convert predictions to binary\n",
    "val_preds_bin = (val_preds > 0.3).astype(int)\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(val_masks.flatten(), val_preds_bin.flatten())\n",
    "f1 = f1_score(val_masks.flatten(), val_preds_bin.flatten())\n",
    "\n",
    "# Calculate Mean IoU\n",
    "mean_iou_metric = tf.keras.metrics.MeanIoU(num_classes=2) \n",
    "mean_iou_metric.update_state(val_masks, val_preds_bin)\n",
    "mean_iou = mean_iou_metric.result().numpy()\n",
    "\n",
    "# Save metrics to Excel\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': [precision],\n",
    "    'F1 Score': [f1],\n",
    "    'Mean IoU': [mean_iou]\n",
    "})\n",
    "print(f\"Precision: \", precision, \"F1 score: \", f1, \"Mean Iou : \", mean_iou)\n",
    "\n",
    "# Plot training history\n",
    "history_df = pd.read_excel('training_history.xlsx')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history_df['loss'], label='Training Loss')\n",
    "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_plot.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history_df['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Save confusion matrix as heatmap\n",
    "cm = confusion_matrix(val_masks.flatten(), val_preds_bin.flatten())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize some predictions on validation set\n",
    "for i in range(3):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].imshow(val_images[i])\n",
    "    axs[0].set_title('Input Image')\n",
    "    axs[1].imshow(val_masks[i].squeeze(), cmap='gray')\n",
    "    axs[1].set_title('Ground Truth')\n",
    "    axs[2].imshow(val_preds_bin[i].squeeze(), cmap='gray')\n",
    "    axs[2].set_title('Predicted Mask')\n",
    "    plt.savefig(f'prediction_{i}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd950f-b291-422c-be72-af9666b703cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate inference time\n",
    "\n",
    "# Use a small batch from validation data\n",
    "sample_images = val_images[:5]\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "predictions = model.predict(sample_images)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = (end_time - start_time) / len(sample_images)\n",
    "print(f\"Average Inference Time per Image: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc9031-635c-47a9-a163-1ccf495b10ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710790e-e9be-4101-9eb9-631123261c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define input size\n",
    "input_size = (512, 512, 3)\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path, target_size):\n",
    "    # Load image\n",
    "    image = load_img(image_path, target_size=target_size[:2])\n",
    "    # Convert to numpy array\n",
    "    image_array = img_to_array(image)\n",
    "    # Normalize the image\n",
    "    image_array = image_array / 255.0\n",
    "    return image_array\n",
    "\n",
    "# Function to save images\n",
    "def save_image(image_array, save_path):\n",
    "    # Convert array to image\n",
    "    image = array_to_img(image_array)\n",
    "    # Save the image\n",
    "    image.save(save_path)\n",
    "\n",
    "# Function to display images\n",
    "def display_images(images, titles=None, cmap='gray'):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(3, 4, i + 1)  # Adjust depending on the number of images\n",
    "        plt.imshow(image, cmap=cmap)\n",
    "        if titles:\n",
    "            plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Load and preprocess images\n",
    "image_folder = '/home/syam.varnatt/THESIS/Plot_Images(x10,x20)'\n",
    "image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith('.png')]\n",
    "\n",
    "# Sort the image paths by filenames\n",
    "image_paths = sorted(image_paths)\n",
    "\n",
    "# Select a subset of images\n",
    "image_paths = image_paths[0:]\n",
    "\n",
    "# Predict segmentation masks for each image\n",
    "# Extract image names from paths\n",
    "image_names = [os.path.basename(path) for path in image_paths]\n",
    "\n",
    "# Define the folder path to save images and masks\n",
    "save_folder = '/home/syam.varnatt/THESIS/attention-unet/aug/results'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Predict segmentation masks and save images and masks\n",
    "predicted_masks = []\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    # Preprocess image\n",
    "    image = preprocess_image(image_path, input_size)\n",
    "    image_batch = np.expand_dims(image, axis=0)\n",
    "    # Predict mask\n",
    "    predicted_mask = model.predict(image_batch)[0]\n",
    "    predicted_mask = (predicted_mask > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # Save the original image\n",
    "    original_image = img_to_array(load_img(image_path, target_size=input_size[:2])) / 255.0\n",
    "    save_image(original_image, os.path.join(save_folder, f'original_{image_names[i]}'))\n",
    "    \n",
    "    # Save the predicted mask\n",
    "    save_image(predicted_mask, os.path.join(save_folder, f'mask_{image_names[i]}'))\n",
    "    \n",
    "    predicted_masks.append(predicted_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76131c85-1497-49fc-980e-c48ecbdd0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate inference time for predicting heights\n",
    "\n",
    "# Use a small batch from validation data\n",
    "sample_images = image_batch[:5]\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "predictions = model.predict(sample_images)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = (end_time - start_time) / len(sample_images)\n",
    "print(f\"Average Inference Time per flight height predicted Image: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659dcc0-272b-4f12-a71b-3874b17c8189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
